{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone Metastasis Classification using ResNet\n",
    "\n",
    "This notebook performs bone metastasis classification using pre-trained ResNet, with all existing layers frozen and all fine-tuned layers unfrozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torchstat import stat\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for random number generation to create reproducible results\n",
    "random_seed = 5\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization according to the ResNet configuration\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get ground truth class of an image\n",
    "def get_img_labels(img_dir):\n",
    "    labels = ''\n",
    "    \n",
    "    for filename in os.listdir(img_dir):\n",
    "        # If image has no metastasis\n",
    "        if (filename[5] == '0'):\n",
    "            labels += filename + \",0\\n\"\n",
    "            \n",
    "        # If image has metastasis\n",
    "        else:\n",
    "            labels += filename + \",1\\n\"\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset-sample-flat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_csv(StringIO(get_img_labels(img_dir)), sep=\",\", header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.img_labels.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "        \n",
    "        # Read and transform image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Determine ground truth class (metastasis or no metastasis)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader for training data\n",
    "dataset = CustomDataset(img_dir=data_dir, transform=preprocess)\n",
    "labels = list(dataset.img_labels.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters; epochs & batch size from Magboo & Abu\n",
    "k_folds = 3 # TODO: Change to 10\n",
    "epochs = 2 # TODO: Change to 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate tensors to the device used for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Performing torch operations on {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "results_accuracy = []\n",
    "results_precision = []\n",
    "results_sensitivity = []\n",
    "results_specificity = []\n",
    "results_f1 = []\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for k folds, with e epochs each\n",
    "for fold, (train_indices, test_indices) in enumerate(kfold.split(dataset)):\n",
    "    print('-' * 50)\n",
    "    print(f'FOLD {fold+1}/{k_folds}:')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    # Sample elements randomly from selected train/test indices\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    test_subsampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "    \n",
    "    # Get DataLoaders for training and test sets\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, \n",
    "                                              batch_size=batch_size, \n",
    "                                              sampler=train_subsampler)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size, \n",
    "                                             sampler=test_subsampler)\n",
    "    \n",
    "    train_losses.append([])\n",
    "    test_losses.append([])\n",
    "    \n",
    "    # Load pre-trained ResNet50 model\n",
    "    model = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\").to(device)\n",
    "\n",
    "    # Fine-tune model: Change last layer to output two classes\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features = model.fc.in_features, out_features = 2),\n",
    "        nn.Softmax(dim = 1))\n",
    "    \n",
    "    # Freeze all layers except the fine-tuned layer\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Define criterion (function used to compute loss) and optimizer for model\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # ---------- Run for x epochs on training set and test set ----------\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                dataloader = trainloader\n",
    "            else:\n",
    "                model.eval()\n",
    "                dataloader = testloader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            total_batch_count = 0\n",
    "\n",
    "            for inputs, labels in dataloader:\n",
    "                # Get the inputs\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                model = model.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # Backpropagation\n",
    "                    optimizer.zero_grad() # Reset gradients from previous passes\n",
    "                    loss.backward() # Compute gradients using derivative of loss\n",
    "                    optimizer.step() # Update values using gradients\n",
    "                \n",
    "                total_batch_count += 1 # Increment number of finished batches\n",
    "                running_loss += loss.item() # Add batch loss to current epoch loss\n",
    "\n",
    "            running_loss /= total_batch_count\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_losses[fold].append(running_loss)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} Training Loss: {running_loss}\")\n",
    "            else:\n",
    "                test_losses[fold].append(running_loss)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} Test Loss: {running_loss}\")\n",
    "                  \n",
    "    # ---------- Get performance metrics for this fold ----------\n",
    "    print('-' * 50)\n",
    "    print(f'FOLD {fold+1}/{k_folds}: Test Results')\n",
    "    print('-' * 50)\n",
    "\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    total = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_data in enumerate(testloader):\n",
    "            # Get the inputs; data is a list of [images, labels]\n",
    "            images, labels = batch_data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Set total and correct\n",
    "            _, predicted = torch.max(input=predictions, dim=1) # Get list of predicted classes\n",
    "            \n",
    "            # Get how many classes there were in this batch\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Get true positive, true negative, false positive, and false negative counts\n",
    "            for index in range(len(labels)):\n",
    "                true_pos = true_pos+1 if (predicted[index] == 1 and labels[index] == 1) else true_pos\n",
    "                true_neg = true_neg+1 if (predicted[index] == 0 and labels[index] == 0) else true_neg\n",
    "                false_pos = false_pos+1 if (predicted[index] == 1 and labels[index] == 0) else false_pos\n",
    "                false_neg = false_neg+1 if (predicted[index] == 0 and labels[index] == 1) else false_neg\n",
    "    \n",
    "    print(f\"TP: {true_pos}, TN: {true_neg}, FP: {false_pos}, FN: {false_neg}, total: {total}\")\n",
    "    \n",
    "    # Get evaluation metrics\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = (true_pos + true_neg)/total if total != 0 else 0\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    results_accuracy.append(accuracy)\n",
    "    \n",
    "    # precision tp / (tp + fp)\n",
    "    precision = true_pos/(true_pos + false_pos) if (true_pos + false_pos) != 0 else 0\n",
    "    print(f\"Precision: {precision}\")\n",
    "    results_precision.append(precision)\n",
    "    \n",
    "    # sensitivity: tp / (tp + fn)\n",
    "    sensitivity = true_pos/(true_pos + false_neg) if (true_pos + false_neg) != 0 else 0\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    results_sensitivity.append(sensitivity)\n",
    "    \n",
    "    # specificity: tn / (tn + fp)\n",
    "    specificity = true_neg/(true_neg + false_pos) if (true_neg + false_pos) != 0 else 0\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    results_specificity.append(specificity)\n",
    "    \n",
    "    # f1: 2(precision * recall)/(precision + recall)\n",
    "    f1 = 2 * (precision * sensitivity)/(precision + sensitivity) if (precision + sensitivity) != 0 else 0\n",
    "    print(f\"F1: {f1}\")\n",
    "    results_f1.append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tensor sizes per layer in model\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights from fine-tuning pre-trained model\n",
    "torch.save(model.state_dict(), 'weights/resnet_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train losses\n",
    "ax = plt.axes()\n",
    "for fold_losses in train_losses:\n",
    "    plt.plot(fold_losses)\n",
    "plt.title('Training Loss Evaluation per fold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.legend(['Fold ' + str(x) for x in range(1, k_folds+1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test losses\n",
    "ax = plt.axes()\n",
    "for fold_losses in test_losses:\n",
    "    plt.plot(fold_losses)\n",
    "plt.title('Test Loss Evaluation per fold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.legend(['Fold ' + str(x) for x in range(1, k_folds+1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance metrics by calculcating average of metrics across all folds\n",
    "print(\"Final Performance Metrics\")\n",
    "print(f\"Accuracy: {np.mean(results_accuracy)}\")\n",
    "print(f\"Precision: {np.mean(results_precision)}\")\n",
    "print(f\"Sensitivity: {np.mean(results_sensitivity)}\")\n",
    "print(f\"Specificity: {np.mean(results_specificity)}\")\n",
    "print(f\"F1: {np.mean(results_f1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_names = [\"0000-0-A.tif\", \"0163-0-P.tif\", \"0198-0-A.tif\"]\n",
    "sample_img_paths = [(data_dir + \"/\" + sample_img_names[i]) for i in range(len(sample_img_names))]\n",
    "sample_classes = [sample_img_names[i][5] for i in range(len(sample_img_names))]\n",
    "sample_ground_truths = [\"No Metastasis\" if sample_classes[i] == 0 else \"Metastasis\" for i in range(len(sample_img_names))]\n",
    "sample_imgs = [Image.open(img_path).convert('RGB') for img_path in sample_img_paths]\n",
    "sample_imgs_show = [Image.open(img_path) for img_path in sample_img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = torch.stack([preprocess(img).to(device) for img in sample_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preds= model(validation_batch).detach().cpu().data.numpy()\n",
    "sample_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(sample_imgs_show), figsize=(20, 5))\n",
    "for i, img in enumerate(sample_imgs_show):\n",
    "    ax = axs[i]\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"Prediction: {:.0f}% No Metastasis, {:.0f}% Metastasis\\n Ground Truth: {}\"\n",
    "                 .format(100*sample_preds[i,0], 100*sample_preds[i,1], sample_ground_truths[i]))\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Due to lack of library support for CUDA, this will error if using CUDA\n",
    "# Get computational complexity\n",
    "model = model.to(\"cpu\")\n",
    "stat(model, (3, 646, 220))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
